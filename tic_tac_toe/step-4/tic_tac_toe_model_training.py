import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Dense, Conv2D, Flatten, Dropout, BatchNormalization,
    LayerNormalization, MultiHeadAttention, GlobalAveragePooling2D,
    Reshape, Concatenate
)
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import (
    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,
    TensorBoard, CSVLogger
)
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import argparse
import os
import json
from glob import glob
from datetime import datetime
import time

# å¤„ç† sklearn ä¾èµ–é—®é¢˜
try:
    from sklearn.metrics import confusion_matrix, classification_report
    import seaborn as sns
    SKLEARN_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: sklearn æˆ– seaborn æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆè¯„ä¼°ã€‚")
    print("å¯ä»¥é€šè¿‡è¿è¡Œ 'pip install scikit-learn seaborn' æ¥å®‰è£…è¿™äº›åº“ã€‚")
    SKLEARN_AVAILABLE = False

def find_latest_npz(data_dir):
    """æŸ¥æ‰¾æŒ‡å®šç›®å½•ä¸­æœ€æ–°çš„ NPZ æ–‡ä»¶"""
    npz_files = glob(os.path.join(data_dir, '*.npz'))
    if not npz_files:
        raise FileNotFoundError(f"ç›®å½• {data_dir} ä¸­æœªæ‰¾åˆ° NPZ æ–‡ä»¶")
    return max(npz_files, key=os.path.getmtime)

def load_data(npz_file):
    """ä» NPZ æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œæ”¯æŒéªŒè¯é›†åˆ’åˆ†"""
    try:
        data = np.load(npz_file, allow_pickle=True)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«éªŒè¯é›†
        if "X_val" in data and "y_val" in data:
            return (
                data["X_train"], data["y_train"], 
                data["X_val"], data["y_val"],
                data["X_test"], data["y_test"]
            )
        else:
            # å‘åå…¼å®¹æ—§æ•°æ®æ–‡ä»¶
            print("âš ï¸ æ•°æ®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°éªŒè¯é›†ã€‚å°†ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºéªŒè¯é›†ã€‚")
            X_train, y_train = data["X_train"], data["y_train"]
            X_test, y_test = data["X_test"], data["y_test"]

            # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œåˆ™ä»è®­ç»ƒé›†ä¸­åˆ†å‰²å‡ºä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†
            val_size = int(len(X_train) * 0.1)  # ä½¿ç”¨ 10% çš„è®­ç»ƒæ•°æ®ä½œä¸ºéªŒè¯é›†
            X_val, y_val = X_train[-val_size:], y_train[-val_size:]
            X_train, y_train = X_train[:-val_size], y_train[:-val_size]

            print(f"å·²ä»è®­ç»ƒé›†åˆ†å‰²å‡º {val_size} ä¸ªæ ·æœ¬ä½œä¸ºéªŒè¯é›†")

            return X_train, y_train, X_val, y_val, X_test, y_test
    except Exception as e:
        print(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        raise

def build_cnn_model(input_shape=(3, 3, 6), dropout_rate=0.3):
    """æ„å»ºç”¨äºäº•å­—æ£‹é¢„æµ‹çš„ CNN æ¨¡å‹"""
    inputs = Input(shape=input_shape)

    # ç¬¬ä¸€ä¸ªå·ç§¯å—
    x = Conv2D(64, (2, 2), activation='relu', padding='same')(inputs)
    x = BatchNormalization()(x)
    x = Conv2D(64, (2, 2), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)

    # ç¬¬äºŒä¸ªå·ç§¯å—
    x = Conv2D(128, (2, 2), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate)(x)

    # å±•å¹³å’Œå…¨è¿æ¥å±‚
    x = Flatten()(x)
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate)(x)

    # è¾“å‡ºå±‚ (èƒœã€è´Ÿã€å¹³)
    outputs = Dense(3, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

def build_transformer_model(input_shape=(3, 3, 6), num_heads=4, ff_dim=128, dropout_rate=0.2):
    """æ„å»ºåŸºäº Transformer çš„äº•å­—æ£‹é¢„æµ‹æ¨¡å‹"""
    inputs = Input(shape=input_shape)

    # å°† 3x3 æ£‹ç›˜é‡å¡‘ä¸ºåºåˆ—
    # ç¬¬ä¸€æ¡è·¯å¾„ï¼šå±•å¹³å¹¶ä½¿ç”¨ transformer å¤„ç†
    x1 = Reshape((input_shape[0] * input_shape[1], input_shape[2]))(inputs)

    # ç›´æ¥ä½¿ç”¨ Dense å±‚è¿›è¡ŒæŠ•å½±ï¼Œé¿å…å¤æ‚çš„ä½ç½®ç¼–ç 
    x1 = Dense(ff_dim)(x1)

    # åº”ç”¨å¤šå¤´æ³¨æ„åŠ›
    attention_output = MultiHeadAttention(
        num_heads=num_heads, key_dim=ff_dim // num_heads
    )(x1, x1)
    attention_output = Dropout(dropout_rate)(attention_output)
    x1 = LayerNormalization(epsilon=1e-6)(x1 + attention_output)

    # å‰é¦ˆç½‘ç»œ
    ffn_output = Dense(ff_dim * 2, activation='relu')(x1)
    ffn_output = Dense(ff_dim)(ffn_output)
    ffn_output = Dropout(dropout_rate)(ffn_output)
    x1 = LayerNormalization(epsilon=1e-6)(x1 + ffn_output)

    # ç¬¬äºŒæ¡è·¯å¾„ï¼šä½¿ç”¨å·ç§¯ç‰¹å¾
    x2 = Conv2D(64, (2, 2), activation='relu', padding='same')(inputs)
    x2 = BatchNormalization()(x2)
    x2 = Conv2D(ff_dim, (2, 2), activation='relu', padding='same')(x2)
    x2 = BatchNormalization()(x2)
    x2 = Flatten()(x2)

    # åˆå¹¶ä¸¤æ¡è·¯å¾„
    combined = Concatenate()([Flatten()(x1), x2])
    combined = Dense(ff_dim, activation='relu')(combined)
    combined = LayerNormalization(epsilon=1e-6)(combined)
    combined = Dropout(dropout_rate)(combined)

    # è¾“å‡ºå±‚
    outputs = Dense(3, activation='softmax')(combined)

    model = Model(inputs=inputs, outputs=outputs)
    return model

def compile_model(model, learning_rate=3e-4, mixed_precision=False):
    """ç¼–è¯‘æ¨¡å‹ï¼Œä½¿ç”¨é€‚å½“çš„ä¼˜åŒ–å™¨å’ŒæŒ‡æ ‡"""
    try:
        if mixed_precision:
            try:
                policy = tf.keras.mixed_precision.Policy('mixed_float16')
                tf.keras.mixed_precision.set_global_policy(policy)
                print("ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
            except:
                print("è­¦å‘Š: è®¾ç½®æ··åˆç²¾åº¦å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤ç²¾åº¦ã€‚")

        optimizer = Adam(learning_rate=learning_rate)

        # åŸºæœ¬æŒ‡æ ‡
        metrics = ['accuracy']

        # å¦‚æœå¯ç”¨ï¼Œæ·»åŠ é«˜çº§æŒ‡æ ‡
        try:
            metrics.extend([
                tf.keras.metrics.AUC(name='auc'),
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall')
            ])
        except:
            print("è­¦å‘Š: æ— æ³•æ·»åŠ é«˜çº§æŒ‡æ ‡ï¼Œä»…ä½¿ç”¨å‡†ç¡®ç‡ã€‚")

        model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=metrics
        )
        return model
    except Exception as e:
        print(f"ç¼–è¯‘æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
        # å°è¯•ä½¿ç”¨åŸºæœ¬é…ç½®
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

def create_callbacks(output_dir, model_name, patience=15):
    """åˆ›å»ºè®­ç»ƒå›è°ƒ"""
    # åˆ›å»ºæ¨¡å‹ç›®å½•
    model_dir = os.path.join(output_dir, model_name)
    os.makedirs(model_dir, exist_ok=True)

    # åŸºæœ¬å›è°ƒ
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=patience,
            restore_best_weights=True,
            verbose=1
        ),
        ModelCheckpoint(
            os.path.join(model_dir, 'best_model.h5'),  # ä½¿ç”¨ .h5 è€Œä¸æ˜¯ .keras ä»¥æé«˜å…¼å®¹æ€§
            save_best_only=True,
            monitor='val_loss',
            verbose=1
        )
    ]

    # å¯é€‰å›è°ƒï¼Œæ ¹æ®å¯ç”¨æ€§æ·»åŠ 
    try:
        callbacks.append(
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=patience // 3,
                min_lr=1e-6,
                verbose=1
            )
        )
    except:
        print("è­¦å‘Š: æ— æ³•æ·»åŠ  ReduceLROnPlateau å›è°ƒã€‚")
    
    try:
        callbacks.append(
            TensorBoard(
                log_dir=os.path.join(model_dir, 'logs'),
                histogram_freq=1,
                write_graph=True
            )
        )
    except:
        print("è­¦å‘Š: æ— æ³•æ·»åŠ  TensorBoard å›è°ƒã€‚")

    try:
        callbacks.append(
            CSVLogger(
                os.path.join(model_dir, 'training_log.csv'),
                separator=',',
                append=False
            )
        )
    except:
        print("è­¦å‘Š: æ— æ³•æ·»åŠ  CSVLogger å›è°ƒã€‚")

    return callbacks

def plot_training_history(history, output_dir, model_name):
    """ç»˜åˆ¶è®­ç»ƒæŒ‡æ ‡å¹¶ä¿å­˜å›¾è¡¨"""
    try:
        # åˆ›å»ºå›¾è¡¨ç›®å½•
        plots_dir = os.path.join(output_dir, model_name, 'plots')
        os.makedirs(plots_dir, exist_ok=True)

        # å‡†ç¡®ç‡å’ŒæŸå¤±å›¾è¡¨
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'], label='è®­ç»ƒ')
        plt.plot(history.history['val_accuracy'], label='éªŒè¯')
        plt.title('æ¨¡å‹å‡†ç¡®ç‡')
        plt.xlabel('è½®æ¬¡')
        plt.ylabel('å‡†ç¡®ç‡')
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.6)

        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'], label='è®­ç»ƒ')
        plt.plot(history.history['val_loss'], label='éªŒè¯')
        plt.title('æ¨¡å‹æŸå¤±')
        plt.xlabel('è½®æ¬¡')
        plt.ylabel('æŸå¤±')
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.6)

        plt.tight_layout()
        plt.savefig(os.path.join(plots_dir, 'accuracy_loss.png'), dpi=300)
        plt.close()

        # å¦‚æœå¯ç”¨ï¼Œç»˜åˆ¶é¢å¤–æŒ‡æ ‡
        if 'auc' in history.history:
            plt.figure(figsize=(15, 5))

            plt.subplot(1, 3, 1)
            plt.plot(history.history['auc'], label='è®­ç»ƒ')
            plt.plot(history.history['val_auc'], label='éªŒè¯')
            plt.title('AUC')
            plt.xlabel('è½®æ¬¡')
            plt.ylabel('AUC')
            plt.legend()
            plt.grid(True, linestyle='--', alpha=0.6)

            plt.subplot(1, 3, 2)
            plt.plot(history.history['precision'], label='è®­ç»ƒ')
            plt.plot(history.history['val_precision'], label='éªŒè¯')
            plt.title('ç²¾ç¡®ç‡')
            plt.xlabel('è½®æ¬¡')
            plt.ylabel('ç²¾ç¡®ç‡')
            plt.legend()
            plt.grid(True, linestyle='--', alpha=0.6)

            plt.subplot(1, 3, 3)
            plt.plot(history.history['recall'], label='è®­ç»ƒ')
            plt.plot(history.history['val_recall'], label='éªŒè¯')
            plt.title('å¬å›ç‡')
            plt.xlabel('è½®æ¬¡')
            plt.ylabel('å¬å›ç‡')
            plt.legend()
            plt.grid(True, linestyle='--', alpha=0.6)

            plt.tight_layout()
            plt.savefig(os.path.join(plots_dir, 'additional_metrics.png'), dpi=300)
            plt.close()
    except Exception as e:
        print(f"ç»˜åˆ¶è®­ç»ƒå†å²æ—¶å‡ºé”™: {str(e)}")
        print("å°†è·³è¿‡å›¾è¡¨ç”Ÿæˆã€‚")

def evaluate_model(model, X_test, y_test, output_dir, model_name):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶ä¿å­˜ç»“æœ"""
    # åˆ›å»ºè¯„ä¼°ç›®å½•
    eval_dir = os.path.join(output_dir, model_name, 'evaluation')
    os.makedirs(eval_dir, exist_ok=True)

    # è¯„ä¼°æ¨¡å‹
    print("\nğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    test_results = model.evaluate(X_test, y_test, verbose=1)

    # è·å–é¢„æµ‹ç»“æœ
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_test, axis=1)

    results = {
        'metrics': {name: float(value) for name, value in zip(model.metrics_names, test_results)}
    }

    # å¦‚æœ sklearn å¯ç”¨ï¼Œç”Ÿæˆæ··æ·†çŸ©é˜µå’Œåˆ†ç±»æŠ¥å‘Š
    if SKLEARN_AVAILABLE:
        try:
            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_true_classes, y_pred_classes)
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                        xticklabels=['èƒœ', 'è´Ÿ', 'å¹³'],
                        yticklabels=['èƒœ', 'è´Ÿ', 'å¹³'])
            plt.xlabel('é¢„æµ‹')
            plt.ylabel('çœŸå®')
            plt.title('æ··æ·†çŸ©é˜µ')
            plt.tight_layout()
            plt.savefig(os.path.join(eval_dir, 'confusion_matrix.png'), dpi=300)
            plt.close()

            # åˆ†ç±»æŠ¥å‘Š
            class_report = classification_report(
                y_true_classes, y_pred_classes,
                target_names=['èƒœ', 'è´Ÿ', 'å¹³'],
                output_dict=True
            )

            # æ·»åŠ åˆ°ç»“æœ
            results['classification_report'] = class_report
            results['confusion_matrix'] = cm.tolist()
        except Exception as e:
            print(f"ç”Ÿæˆé«˜çº§è¯„ä¼°æŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
    else:
        # ç®€å•çš„æ··æ·†çŸ©é˜µï¼Œä¸ä½¿ç”¨ sklearn
        cm = np.zeros((3, 3), dtype=int)
        for i, j in zip(y_true_classes, y_pred_classes):
            cm[i, j] += 1

        # æ·»åŠ åˆ°ç»“æœ
        results['confusion_matrix'] = cm.tolist()

        # æ‰‹åŠ¨è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
        precision = np.zeros(3)
        recall = np.zeros(3)

        for i in range(3):
            # ç²¾ç¡®ç‡ï¼šæ­£ç¡®é¢„æµ‹ä¸ºç±»åˆ« i çš„æ ·æœ¬ / é¢„æµ‹ä¸ºç±»åˆ« i çš„æ ·æœ¬æ€»æ•°
            if np.sum(cm[:, i]) > 0:
                precision[i] = cm[i, i] / np.sum(cm[:, i])

            # å¬å›ç‡ï¼šæ­£ç¡®é¢„æµ‹ä¸ºç±»åˆ« i çš„æ ·æœ¬ / ç±»åˆ« i çš„æ ·æœ¬æ€»æ•°
            if np.sum(cm[i, :]) > 0:
                recall[i] = cm[i, i] / np.sum(cm[i, :])

        # æ·»åŠ åˆ°ç»“æœ
        results['manual_metrics'] = {
            'precision': precision.tolist(),
            'recall': recall.tolist()
        }

    # ä¿å­˜ç»“æœ
    with open(os.path.join(eval_dir, 'evaluation_results.json'), 'w') as f:
        json.dump(results, f, indent=4)

    return results

def save_model_summary(model, output_dir, model_name):
    """å°†æ¨¡å‹æ¶æ„æ‘˜è¦ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶"""
    try:
        summary_path = os.path.join(output_dir, model_name, 'model_summary.txt')

        # é‡å®šå‘ stdout ä»¥æ•è·æ‘˜è¦
        import io
        import sys
        original_stdout = sys.stdout
        captured_output = io.StringIO()
        sys.stdout = captured_output

        # æ‰“å°æ¨¡å‹æ‘˜è¦
        model.summary()

        # æ¢å¤ stdout å¹¶ä¿å­˜æ•è·çš„è¾“å‡º
        sys.stdout = original_stdout
        with open(summary_path, 'w') as f:
            f.write(captured_output.getvalue())
    except Exception as e:
        print(f"ä¿å­˜æ¨¡å‹æ‘˜è¦æ—¶å‡ºé”™: {str(e)}")

def build_ensemble_model(input_shape, dropout_rate=0.2):
    """æ„å»º CNN å’Œ Transformer çš„é›†æˆæ¨¡å‹"""
    try:
        # åˆ›å»ºå•ç‹¬çš„ CNN å’Œ Transformer æ¨¡å‹
        cnn_model = build_cnn_model(input_shape=input_shape, dropout_rate=dropout_rate)
        transformer_model = build_transformer_model(
            input_shape=input_shape,
            dropout_rate=dropout_rate
        )

        # åˆ›å»ºé›†æˆè¾“å…¥
        inputs = Input(shape=input_shape)
    
        # è·å–ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡º
        cnn_output = cnn_model(inputs)
        transformer_output = transformer_model(inputs)
    
        # åˆå¹¶è¾“å‡º
        combined = Concatenate()([cnn_output, transformer_output])
        combined = Dense(32, activation='relu')(combined)
        combined = BatchNormalization()(combined)
        combined = Dropout(dropout_rate)(combined)
        outputs = Dense(3, activation='softmax')(combined)

        model = Model(inputs=inputs, outputs=outputs)
        return model
    except Exception as e:
        print(f"æ„å»ºé›†æˆæ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
        print("è¿”å›å¤‡ç”¨ CNN æ¨¡å‹...")
        return build_cnn_model(input_shape=input_shape, dropout_rate=dropout_rate)

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è®­ç»ƒäº•å­—æ£‹é¢„æµ‹æ¨¡å‹')
    parser.add_argument('--data_dir', type=str, default='tic_tac_toe_advanced_data/npz_data',
                        help='åŒ…å« NPZ æ•°æ®æ–‡ä»¶çš„ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='tic_tac_toe_models',
                        help='ä¿å­˜æ¨¡å‹è¾“å‡ºçš„ç›®å½•')
    parser.add_argument('--model_type', type=str, default='cnn',
                        choices=['cnn', 'transformer', 'ensemble'],
                        help='è¦è®­ç»ƒçš„æ¨¡å‹ç±»å‹')
    parser.add_argument('--epochs', type=int, default=100,
                        help='è®­ç»ƒè½®æ¬¡æ•°')
    parser.add_argument('--batch_size', type=int, default=128,
                        help='è®­ç»ƒæ‰¹é‡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=3e-4,
                        help='åˆå§‹å­¦ä¹ ç‡')
    parser.add_argument('--patience', type=int, default=15,
                        help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--dropout', type=float, default=0.2,
                        help='Dropout ç‡')
    parser.add_argument('--mixed_precision', action='store_true',
                        help='ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    parser.add_argument('--data_file', type=str, default=None,
                        help='è¦ä½¿ç”¨çš„ç‰¹å®š NPZ æ–‡ä»¶ï¼ˆå¦åˆ™ä½¿ç”¨æœ€æ–°çš„ï¼‰')
    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    # è®¾ç½® TensorFlow å†…å­˜å¢é•¿ä»¥é¿å… OOM é”™è¯¯
    try:
        physical_devices = tf.config.list_physical_devices('GPU')
        if physical_devices:
            for device in physical_devices:
                tf.config.experimental.set_memory_growth(device, True)
            print(f"ğŸ’» æ‰¾åˆ° {len(physical_devices)} ä¸ª GPUã€‚å·²å¯ç”¨å†…å­˜å¢é•¿ã€‚")
        else:
            print("âš ï¸ æœªæ‰¾åˆ° GPUã€‚åœ¨ CPU ä¸Šè®­ç»ƒã€‚")
    except:
        print("âš ï¸ è®¾ç½® GPU å†…å­˜å¢é•¿å¤±è´¥ã€‚")

    # æŸ¥æ‰¾å¹¶åŠ è½½æ•°æ®
    try:
        if args.data_file:
            npz_file = args.data_file
        else:
            npz_file = find_latest_npz(args.data_dir)

        print(f"ğŸ“‚ ä»ä»¥ä¸‹ä½ç½®åŠ è½½æ•°æ®: {npz_file}")
        X_train, y_train, X_val, y_val, X_test, y_test = load_data(npz_file)

        print(f"è®­ç»ƒé›†: {X_train.shape[0]} ä¸ªæ ·æœ¬")
        print(f"éªŒè¯é›†: {X_val.shape[0]} ä¸ªæ ·æœ¬")
        print(f"æµ‹è¯•é›†: {X_test.shape[0]} ä¸ªæ ·æœ¬")
        print(f"è¾“å…¥å½¢çŠ¶: {X_train.shape[1:]}")
    except Exception as e:
        print(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        exit(1)

    # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºæ¨¡å‹åç§°
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_name = f"{args.model_type}_{timestamp}"

    # æ„å»ºæ¨¡å‹
    print(f"ğŸ—ï¸ æ„å»º {args.model_type.upper()} æ¨¡å‹...")
    try:
        if args.model_type == 'cnn':
            model = build_cnn_model(input_shape=X_train.shape[1:], dropout_rate=args.dropout)
        elif args.model_type == 'transformer':
            model = build_transformer_model(
                input_shape=X_train.shape[1:],
                dropout_rate=args.dropout
            )
        elif args.model_type == 'ensemble':
            model = build_ensemble_model(
                input_shape=X_train.shape[1:],
                dropout_rate=args.dropout
            )
    except Exception as e:
        print(f"æ„å»ºæ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
        print("å°†ä½¿ç”¨ CNN æ¨¡å‹ä½œä¸ºå¤‡ç”¨...")
        model = build_cnn_model(input_shape=X_train.shape[1:], dropout_rate=args.dropout)

    # ç¼–è¯‘æ¨¡å‹
    model = compile_model(model, learning_rate=args.learning_rate, mixed_precision=args.mixed_precision)

    # ä¿å­˜æ¨¡å‹æ‘˜è¦
    save_model_summary(model, args.output_dir, model_name)

    # åˆ›å»ºå›è°ƒ
    callbacks = create_callbacks(args.output_dir, model_name, patience=args.patience)

    # è®­ç»ƒæ¨¡å‹
    print(f"ğŸš€ è®­ç»ƒ {args.model_type.upper()} æ¨¡å‹ï¼Œè½®æ¬¡æ•°: {args.epochs}...")
    start_time = time.time()

    try:
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=args.epochs,
            batch_size=args.batch_size,
            callbacks=callbacks,
            verbose=1
        )

        training_time = time.time() - start_time
        print(f"â±ï¸ è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ {training_time:.2f} ç§’")

        # ç»˜åˆ¶è®­ç»ƒå†å²
        plot_training_history(history, args.output_dir, model_name)

        # è¯„ä¼°æ¨¡å‹
        results = evaluate_model(model, X_test, y_test, args.output_dir, model_name)

        # æ‰“å°æœ€ç»ˆç»“æœ
        print("\nğŸ“ˆ æœ€ç»ˆç»“æœ:")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {results['metrics']['accuracy']:.4f}")
        print(f"æµ‹è¯•æŸå¤±: {results['metrics']['loss']:.4f}")

        # ä¿å­˜æ¨¡å‹å…ƒæ•°æ®
        metadata = {
            'model_type': args.model_type,
            'input_shape': list(X_train.shape[1:]),
            'data_file': npz_file,
            'training_samples': X_train.shape[0],
            'validation_samples': X_val.shape[0],
            'test_samples': X_test.shape[0],
            'epochs': args.epochs,
            'batch_size': args.batch_size,
            'learning_rate': args.learning_rate,
            'training_time': training_time,
            'test_results': results['metrics'],
            'timestamp': timestamp,
            'tensorflow_version': tf.__version__
        }

        with open(os.path.join(args.output_dir, model_name, 'metadata.json'), 'w') as f:
            json.dump(metadata, f, indent=4)

        # å°è¯•ä¿å­˜æ¨¡å‹
        try:
            model_path = os.path.join(args.output_dir, model_name, 'final_model.h5')
            model.save(model_path)
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
        except Exception as e:
            print(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            try:
                # å°è¯•ä½¿ç”¨å¦ä¸€ç§æ ¼å¼ä¿å­˜
                model_path = os.path.join(args.output_dir, model_name, 'final_model')
                model.save(model_path)
                print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
            except:
                print("æ— æ³•ä¿å­˜æ¨¡å‹ã€‚")

        print(f"âœ… æ¨¡å‹å’Œè¾“å‡ºå·²ä¿å­˜åˆ° {os.path.join(args.output_dir, model_name)}")

    except Exception as e:
        print(f"è®­ç»ƒæ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
        print("è¯·æ£€æŸ¥æ‚¨çš„æ•°æ®å’Œæ¨¡å‹é…ç½®ã€‚")

if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"æ‰§è¡Œæ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
